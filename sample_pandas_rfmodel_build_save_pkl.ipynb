{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample_pandas_rfmodel_build_save_pkl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P13wQ-p07Udm"
      },
      "source": [
        "# Import required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.common import flatten\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3zin3v7VR8"
      },
      "source": [
        "spark = SparkSession.builder.appName('Pandas_Model').enableHiveSupport().getOrCreate()\n",
        "\n",
        "df = spark.read.parquet('parquet_file_path')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5VPW1rM7WWR"
      },
      "source": [
        "#Columns selection\n",
        "usecols = ['FLAG','ITEM_PRICE_AMT','All_columns_that_needs_tobe_used_in_model']\n",
        "\n",
        "cat_cols = ['VISIT_DEVICE_TYPE', 'All_categorical_cols']\n",
        "\n",
        "num_cols = ['ITEM_PRICE_AMT', 'All_num_cols']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1-gjEdN7Wsv"
      },
      "source": [
        "#Preprocessing function\n",
        "def data_preprocessor(df):\n",
        "  \"\"\"\n",
        "  Return the preprocessed dataframe after\n",
        "  imputing the missing values and encoding\n",
        "  categorical variables\n",
        "\n",
        "  :param df: dataframe to be processed for imputation and encoding\n",
        "  :return: preprocessed dataframe\n",
        "  \"\"\"\n",
        "  df = df.select(usecols).toPandas()\n",
        "  for col in num_cols:\n",
        "    df[col] = pd.to_numeric(df[col])\n",
        "\n",
        "  X = pd.DataFrame(df.drop(['FLAG'], axis = 1))\n",
        "  y = pd.DataFrame(df['FLAG'])\n",
        "\n",
        "  numeric_features = [var for var in X.columns if X[var].dtype != 'O']\n",
        "  categorical_features = [var for var in X.columns if X[var].dtype == 'O']\n",
        "\n",
        "  X = X[numeric_features + categorical_features]\n",
        "  cols = numeric_features + categorical_features\n",
        "  median_price = X.ITEM_PRICE_AMT.median()\n",
        "  imputer = ColumnTransformer(transformers = [\n",
        "                                              ('num_imputation', SimpleImputer(strategy = 'constant', fill_value = median_price),[0]),\n",
        "                                              ('cat_imputation', SimpleImputer(missing_values= None, strategy ='constant', fill_value='missing'),\n",
        "                                               slice(1,6))])\n",
        "  X = imputer.fit_transform(X.values)\n",
        "  X = pd.DataFrame(X, columns=cols)\n",
        "  categorical_mappings = {}\n",
        "\n",
        "  for var in categorical_features:\n",
        "    categorical_mappings[var] = (X.join(Y)).groupby([var])['FLAG'].mean().to_dict()\n",
        "\n",
        "  for var in categorical_features:\n",
        "    X[var] = X[var].map(categorical_mappings[var])\n",
        "\n",
        "  df = pd.concat([X.reset_index(drop = True), Y], axis=1)\n",
        "  return df\n",
        "  \n",
        "preprocessed_df = data_preprocessor(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pnW9GmH7Xu5"
      },
      "source": [
        "#Over Sampling\n",
        "def over_sampling(preprocessed_df):\n",
        "  \"\"\"\n",
        "  Use SMOTE technique to oversample\n",
        "  The train data for handling unbalanced classes\n",
        "  after a train test split in 70:30 ratio\n",
        "\n",
        "  :param preprocessed_df: preprocessed dataframe after imputations and encoding\n",
        "  :return: train dataframe with balanced distribution of classes and test dataframe\n",
        "  \"\"\"\n",
        "  X = pd.DataFrame(preprocessed_df.drop(['FLAG'], axis = 1))\n",
        "  Y = pd.DataFrame(preprocessed_df['FLAG'])\n",
        "\n",
        "  Y = Y.values.ravel()\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y ,test_size=0.3, random_state=0)\n",
        "  smt = SMOTE(random_state = 42)\n",
        "\n",
        "  X_train_res, Y_train_res = smt.fit_resample(X_train, Y_train)\n",
        "  Y_train_res = pd.DataFrame({'FLAG': Y_train_res[:]})\n",
        "\n",
        "  train_df = pd.concat([X_train_res.reset_index(drop = True), Y_train_res], axis = 1)\n",
        "  Y_test = pd.DataFrame({'FLAG': Y_test[:]})\n",
        "  test_df = pd.concat([X_test.reset_index(drop = True), Y_test], axis = 1)\n",
        "  return train_df, test_df\n",
        "\n",
        "sampled_df, test_df = over_sampling(preprocessed_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwMHCvBj8A32"
      },
      "source": [
        "# Hyper Parameter Tuning\n",
        "def hyper_parameter_tuning(sampled_df):\n",
        "  \"\"\"\n",
        "  Perform tuning using\n",
        "  GridSearchCV for random forest clasifier\n",
        "  :param sampled_df: train dataframe with balanced classes after oversampling\n",
        "  :return: best set of hyperparameters\n",
        "  \"\"\"\n",
        "  n_estimators = [300,500,750,800]\n",
        "  max_depth = [5,8, None]\n",
        "  min_samples_split = [2,5]\n",
        "  min_samples_leaf = [1,2]\n",
        "  forest = RandomForestClassifier(random_state=1)\n",
        "  folds = 5\n",
        "  skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state=1)\n",
        "  kf = KFold(n_splits=folds, shuffle = True, random_state=1)\n",
        "  hyperF = dict(n_estimators = n_estimators,\n",
        "                max_depth = max_depth,\n",
        "                min_samples_split = min_samples_split,\n",
        "                min_samples_leaf = min_samples_leaf)\n",
        "  gridF = GridSearchCV(forest, hyperF, cv=skf, verbose=1, n_jobs=-1, scoring='recall')\n",
        "  X_train_res = pd.DataFrame(sampled_df.drop(['FLAG'], axis =1))\n",
        "  Y_train_res = pd.DataFrame(sampled_df['FLAG']).values.ravel()\n",
        "  best_ft = gridF.fit(X_train_res, Y_train_res)\n",
        "  best_ft_param = best_ft.best_params_\n",
        "  return best_ft_param\n",
        "\n",
        "best_ft_param = hyper_parameter_tuning(sampled_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GFPGa6n8AbZ"
      },
      "source": [
        "# Model Training\n",
        "def train_model(sampled_df):\n",
        "  \"\"\"\n",
        "  Train a random Forest Classifier\n",
        "  Using the best params out of hyperparameter tuning\n",
        "  :param: sampled_df: train dataframe for performing the training\n",
        "  :return: trained model object\n",
        "  \"\"\"\n",
        "  X_train_res = pd.DataFrame(sampled_df.drop(['FLAG'], axis=1))\n",
        "  Y_train_res = pd.DataFrame(sampled_df['FLAG']).values.ravel()\n",
        "  rf_model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight = None,\n",
        "                                    criterion= 'gini',max_depth=best_ft_param['max_depth'],max_features='auto',\n",
        "                                    min_samples_leaf=best_ft_param['min_samples_leaf'],\n",
        "                                    min_samples_split = best_ft_param['min_samples_split'],\n",
        "                                    n_estimators = best_ft_param['n_estimators'],oob_score = False)\n",
        "  rf_model1 = rf_model.fit(X_train_res, Y_train_res)\n",
        "  return rf_model1\n",
        "  \n",
        "rf_model1 = train_model(sampled_df)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w6L6aE37---"
      },
      "source": [
        "#Metrics for predictions made on test set\n",
        "def get_model_metrics_test_set():\n",
        "  \"\"\"\n",
        "  Uses test set to evaluate model metrics and\n",
        "  print precision, recall and F1\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  X_test = pd.DataFrame(test_df.drop(['FLAG'], axis=1))\n",
        "  Y_test = pd.DataFrame(test_df['FLAG'])\n",
        "  predictions = rf_model1.predict(X_test)\n",
        "  rf_model1.score(X_test, Y_test)\n",
        "  Y_pred = rf_model1.predict_proba(X_test)\n",
        "  Y_pred_v2 = np.where(Y_pred[:,1] >=0.5, 1, 0)\n",
        "  print('Accuracy on Test set: {:.2f}'.format(rf_model1.score(X_test, Y_test)))\n",
        "  print(confusion_matrix(Y_test, Y_pred_v2))\n",
        "  print(classification_report(Y_test, Y_pred_v2))\n",
        "\n",
        "get_model_metrics_test_set()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kerDqMCU8QrQ"
      },
      "source": [
        "#Saving Serialized model\n",
        "def save_serialised_model(model_object, pkl_file_name):\n",
        "  \"\"\"\n",
        "  save a serialised object of the trained model\n",
        "\n",
        "  :param: model_object: trained model object\n",
        "  :param: pkl_file_name: string, name of the pickled file to serialise the object to\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  pickle.dump(model_object, open(pkl_file_name, 'wb'))\n",
        "\n",
        "save_serialised_model(rf_model1, 'trained_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}