{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "individual-apartment",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "improved-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_month = '2021-05-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-stationery",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df1 = spark.sql(\"\"\" SELECT DISTINCT CUST_ID,ACCT_NUM,PYMNT_DT,RPT_MTH FROM (\n",
    "SELECT DISTINCT B.CUST_ID,B.ACCT_NUM,PYMNT_DT,RPT_MTH,\n",
    "ROW_NUMBER() OVER (PARTITION BY B.CUST_ID, B.ACCT_NUM ORDER BY PYMNT_DT DESC) AS RN\n",
    "FROM TABLE_NAME B INNER JOIN ANOTHER_TABLE CA\n",
    "ON B.CUST_ID=CA.CUST_ID AND B.ACCT_NUM=CA.ACCT_NUM \n",
    "WHERE B.FIN_UPG_FLAG= 'Y' -- AND OTHER FILTERS\n",
    "AND B.PYMNT_DT BETWEEN date_format('{0}','yyyy-MM-dd') and last_day('{0}')\n",
    "AND (CA.ACCT_TERM_DT IS NULL OR CA.ACCT_TERM_DT > last_day(add_months('{0}', -1)))\n",
    "AND B.RPT_MTH = date_format('{0}', 'yyyy-MM-01'))\n",
    "WHERE RN=1\"\"\".format(report_month))\n",
    "df1.createOrReplaceTempView('Base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(distinct cust_id||acct_num) from base').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-problem",
   "metadata": {},
   "source": [
    "# Customer purchased Accessories(Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df4 = spark.sql(\"\"\"SELECT * FROM (\n",
    "SELECT DISTINCT CUST_ID,ACCT_NUM,EQP_CLASS_DESC,PYMNT_DT, 1 AS ACCESSORY,\n",
    "ROW_NUMBER() OVER (PARTITION BY CUST_ID,ACCT_NUM ORDER BY PYMNT_DT DESC) AS RNUM\n",
    "FROM TABLE_NAME WHERE \n",
    "EQP_CLASS_DESC = 'Accessories' AND\n",
    "SLS_DIST_CHNL_TYPE_CD = 'N' AND\n",
    "PYMNT_DT BETWEEN date_format('{0}','yyyy-MM-dd') AND last_day('{0}')\n",
    "AND rpt_mth = '{0}')\n",
    "WHERE RNUM=1\"\"\".format(report_month))\n",
    "df4.createOrReplaceTempView('accessories_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-office",
   "metadata": {},
   "source": [
    "# Join Base & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df5 = spark.sql(\"\"\"SELECT A.*,B.ACCESSORY ACC_FLAG FROM BASE A LEFT JOIN ACCESSORIES_BASE B\n",
    "ON A.CUST_ID = B.CUST_ID AND A.ACCT_NUM = B.ACCT_NUM AND A.PYMNT_DT = B.PYMNT_DT\"\"\".format(report_month))\n",
    "df5.createOrReplaceTempView('acc_base')\n",
    "print(df5.count(), len(df5.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-indie",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-nursery",
   "metadata": {},
   "source": [
    "# 1st Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = spark.sql(\"\"\"SELECT * FROM (\n",
    "SELECT DISTINCT A.CUST_ID,A.ACCT_NUM,COALESCE(B.TOTAL_LINES_ON_ACCT,0) AS TOTAL_LINES_ON_ACCT,\n",
    "COALESCE(B.ACCT_ACTIVE_LOAN_CNT,0) AS ACCT_ACTIVE_LOAN_CNT,\n",
    "COALESCE(B.ACCT_TENURE_MNTHS,0) AS ACCT_TENURE_MTHS,\n",
    "ROW_NUMBER() OVER (PARTITION BY A.CUST_ID,A.ACCT_NUM ORDER BY date_format('{0}','yyyy-MM-dd') desc) AS RNUM\n",
    "FROM ACC_BASE A LEFT JOIN FEATURE_TABLE B\n",
    "ON A.CUST_ID = B.CUST_ID\n",
    "WHERE B.RPT_MTH = date_format(add_months('{0}',-1), 'yyyy-MM-01')\n",
    "AND B.BASE_MTH IS NOT NULL) AB\n",
    "WHERE RNUM=1\"\"\".format(report_month))\n",
    "df6.createorReplaceTempView('FEATURE_ENTITY1')\n",
    "df6.printSchema()\n",
    "print(\"Rown & Column:\\n\", df6.count(), len(df6.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-variance",
   "metadata": {},
   "source": [
    "# 2nd Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df8 = spark.sql(\"\"\"SELECT * FROM (\n",
    "SELECT A.CUST_ID,A.ACCT_NUM,COALESCE(B.REV_LAST_2YR_PURCHASE,0) AS REV_LAST_2YR_PURCHASE,\n",
    "COALESCE(B.TOT_ACCSSRY_PURCHASE,0) AS TOT_ACCSSRY_PURCHASE,\n",
    "ROW_NUMBER() OVER(PARTITION BY A.CUST_ID,A.ACCT_NUM ORDER BY '{0}' DESC) AS RN1\n",
    "FROM ACC_BASE A\n",
    "LEFT JOIN\n",
    "(SELECT * FROM \n",
    "  (SELECT CUST_ID,ACCT_NUM,SLS_DIST_CHNL_TYPE_CD,SUM(ITEM_PRICE_AMT) AS REV_LAST_2YR_PURCHASE,\n",
    "  SUM(NET_SALES) AS TOT_ACCSSRY_PURCHASE\n",
    "  FROM TABLE WHERE\n",
    "  LOWER(EQP_CLASS_DESC) = 'accessories' AND \n",
    "  PYMNT_DT>=date_format(add_months('{0}',-24),'yyyy-MM-dd') AND PYMNT_DT<date_format('{0}','yyyy-MM-dd')\n",
    "  AND RPT_MTH<'{0}'\n",
    "  GROUP BY 1,2,3) AB\n",
    "  WHERE TOT_ACCSSRY_PURCHASE>0)B\n",
    "  ON A.CUST_ID=B.CUST_ID AND A.ACCT_NUM=B.ACCT_NUM)X\n",
    "  WHERE RN1=1\"\"\".format(report_month))\n",
    "df8.createOrReplaceTempView('FEATURE_ENTITY2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-rough",
   "metadata": {},
   "source": [
    "# Join all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df200 = spark.sql(\"\"\"\n",
    "SELECT DISTINCT A.CUST_ID,A.ACCT_NUM,A.ACC_FLAG,\n",
    "COALESCE(B.TOTAL_LINES_ON_ACCT,0) AS TOTAL_LINES_ON_ACCT,\n",
    "COALESCE(B.ACCT_ACTIVE_LOAN_CNT,0) AS ACCT_ACTIVE_LOAN_CNT,\n",
    "COALESCE(B.ACCT_TENURE_MTHS,0) AS ACCT_TENURE_MTHS,\n",
    "COALESCE(ROUND(D.REV_LAST_2YR_PURCHASE,2),0) AS REV_LAST_2YR_PURCHASE,\n",
    "COALESCE(ROUND(D.TOT_ACCSSRY_PURCHASE,2),0) AS TOT_ACCSSRY_PURCHASE\n",
    "FROM ACC_BASE A LEFT JOIN FEATURE_ENTITY1 B\n",
    "ON A.CUST_ID = B.CUST_ID AND A.ACCT_NUM = B.ACCT_NUM\n",
    "LEFT JOIN FEATURE_ENTITY2 D\n",
    "ON A.CUST_ID = D.CUST_ID AND A.ACCT_NUM = D.ACCT_NUM\n",
    "\"\"\")\n",
    "df200.createOrReplaceTempView('FINAL_DATASET')\n",
    "df200.printSchema()\n",
    "print(\"Rows & Columns:\\n\", df200.count(), len(df200.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-pacific",
   "metadata": {},
   "source": [
    "# Write to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = spark.sql(\"SELECT * FROM FINAL_DATASET\")\n",
    "d.write.parquet(\"path/final_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-genius",
   "metadata": {},
   "source": [
    "# Grant Read only access to all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -chmod -R 777 'file_path'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
