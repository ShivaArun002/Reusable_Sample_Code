{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scoring_file_Sample_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgDXDSwPqf5t"
      },
      "source": [
        "# Importing Packages\n",
        "import datetime\n",
        "import pickle\n",
        "from getpass import getuser\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (classification_report,precision_score,recall_score,roc_auc_score,f1_score,confusion_matrix)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import (Bucketizer,QuantileDiscretizer,VectorAssembler,StandardScaler,OneHotEncoder,StringIndexer)\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.classification import GBTClassifier,GBTClassificationModel\n",
        "from pyspark.mllib.evaluation import (BinaryClassificationMetrics,MulticlassMetrics)\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJdwAZXqq2QO"
      },
      "source": [
        "sc = SparkContext.getOrCreate()\n",
        "hivecontext = HiveContext(sc)\n",
        "hivecontext.setConf(\"hive.exec.dynamic.partition.mode\",\"nonstrict\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpA4PrpJrfoo"
      },
      "source": [
        "modelid = 'accessorypurchasedig' # Model ID to be used\n",
        "spark = SparkSession.builder.enableHiveSupport().appName(modelid).getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A5hFwUTryxR"
      },
      "source": [
        "# Parser arguments addition\n",
        "if len(sys.argv) >= 4:\n",
        "  parser = argparse.ArgumentParser(description = \"My Parser\")\n",
        "  parser.add_argument(\"--env\", default='', help = \"db details.\")\n",
        "  parser.add_argument(\"--db_feat_data\", default = '' , help = \"input db.\")\n",
        "  parser.add_argument(\"--tbl_feat_data\", default ='', help =\"input table.\")\n",
        "  parser.add_argument(\"--db_pred_data\", default='', help = \"output db.\")\n",
        "  parser.add_argument(\"--tbl_pred_data\", default ='', help \"output table.\")\n",
        "  print(sys.argv)\n",
        "  parsed_args = parser.parse_args(sys.argv[1:])\n",
        "  envnmnt = parsed_args.env\n",
        "  intable = parsed_args.db_feat_data + \".\" + parsed_args.tbl_feat_data\n",
        "  outtable =parsed_args.db_pred_data + \".\" + parsed_args.tbl_pred_data\n",
        "else:\n",
        "  sys.exit(\"no env passed: pass input features db & table, output predictions db & table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fSX0_WKuBRc"
      },
      "source": [
        "# Loading Saved pickle file\n",
        "serialized_object_path = \"hdfs:///user/ashiva/pipelineModel_obj.file\" # Saved Serialized objects path (Such as median/mean value imputations)\n",
        "propensity_model_path = \"hdfs:///user/ashiva/pipelineModel.file\" # Path of the trained Model file \n",
        "print('Propensity_Model_Path-->', propensity_model_path)\n",
        "pipeline_model = PipelineModel.load(propensity_model_path) # Loading the pre trained Model\n",
        "print('serialized_object_path-->', serialized_object_path)\n",
        "serialized_objects_file = tf.io.gfile.GFile(serialized_object_path, 'rb') # converting serialized objects to GFile\n",
        "serialized_objects = pickle.load(serialized_objects_file) # Loading Serialized objects using pickle\n",
        "serialized_objects_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9kX46PjvunL"
      },
      "source": [
        "# Data Preprocessing\n",
        "def pre_processing_(data_df , serialized_objects):\n",
        "  \"\"\"\n",
        "    preprocess the test dataframe to be passed to model and get predictions\n",
        "    :param data_df: dataframe to be preprocessed\n",
        "    :param serialized_objects: dictionary to impute null values during prediction\n",
        "    :return : preprocessed dataframe\n",
        "    \"\"\"\n",
        "  max_recency_acc_dig = serialized_objects['max_recency_acc_dig'] # These values are taken from trained model values\n",
        "  max_recency_dig_2yr = serialized_objects['max_recency_dig_2yr'] # These values are taken from trained model values\n",
        "  max_acc_recency_mf  = serialized_objects['max_acc_recency_mf'] #These are values imported in training dataset. Same values needs to be used to impute missing values in unseen data\n",
        "\n",
        "  data_df = data_df.na.fill({\n",
        "      'recency_acc_dig' : max_recency_acc_dig, # Filling missing values\n",
        "      'recency_dig_2yr' : max_recency_dig_2yr,\n",
        "      'acc_recency_mf'  : max_acc_recency_mf\n",
        "  })\n",
        "\n",
        "  freq_acc_upg_2yrs_split = [-float('inf'), 0, 1, 2, float('inf')]\n",
        "  bucketizer_freq_acc_upg_2yrs = Bucketizer(splits=freq_acc_upg_2yrs_split, inputCol='freq_acc_upg_acc_2yrs', outputCol='freq_acc_upg_acc_2yrs_bkt')\n",
        "  data_df = bucketizer_freq_acc_upg_2yrs.setHandleInvalid('keep').transform(data_df) # Binning the freq_acc_upg_acc_2yrs column\n",
        "\n",
        "  tot_purchase_split = [-float('inf'), 0, 1, 2, 3, float('inf')]\n",
        "  bucketizer_tot_purchase = Bucketizer(splits=tot_purchase_split, inputCol='tot_accsry_purchse', outputCol='tot_accsry_purchse_bkt')\n",
        "  data_df = bucketizer_tot_purchase.setHandleInvalid('keep').transform(data_df) # Binning the tot_accsry_purchse column\n",
        "\n",
        "  del_cols_new = ['freq_acc_upg_acc_2yrs', 'tot_accsry_purchse']\n",
        "  data_df = data_df.drop(*del_cols_new) # Dropping the older continuous columns\n",
        "  return data_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHYvpxWa2AEb"
      },
      "source": [
        "# Loading Features Data\n",
        "feat_df = spark.sql('SELECT * FROM {}'.format(intable))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dyxE0ie2MhK"
      },
      "source": [
        "# Defining Predictions Function\n",
        "def propensity_predictions(pipeline_model, serialized_objects, df, spark, hiveContext, outtable):\n",
        "  \"\"\"\n",
        "    get predictions for preprocessed data frame and write them into the final table\n",
        "    :param pipeline_model: pretrained model instance\n",
        "    :param serialized_objects: dictionary to impute null values during prediction\n",
        "    :param df : preprocessed data frame\n",
        "    :param spark : Spark Context\n",
        "    :param hiveContext : hiveContext\n",
        "    :outtable : Final table to which the predictions need to be written into\n",
        "    :return nothing\n",
        "    \"\"\"\n",
        "  processed_df = pre_processing_(data_df = df, serialized_objects = serialized_objects)\n",
        "  predictions = pipeline_model.transform(processed_df)\n",
        "  finalCols = ['sor_id', 'cust_id', 'acct_num', 'rpt_mth', 'score_model_id', 'score_value', 'score_decile',\n",
        "               'score_centile', 'area_cd', 'score_type', 'orig_score_value', 'high_score_ind'] # Mandatory columns that should be present in outtable i.e., predictions table\n",
        "  to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType())) \n",
        "  prop_final_df = predictions.withColumn('probability_value', to_array('probability')) . \\\n",
        "                                  withColumn('score_value', predictions['prediction'])\n",
        "\n",
        "  prop_final_df = prop_final_df.withColumn('probability', prop_final_df.probability_value[1])\n",
        "  discretizer_decile = QuantileDiscretizer(numBuckets =10, inputCol ='probability', outputCol='score_decile',relativeError=0.0)\n",
        "  discretizer_centile = QuantileDiscretizer(numBuckets =100, inputCol ='probability', outputCol='score_centile',relativeError=0.0)\n",
        "  prop_final_df = discretizer_decile.fit(prop_final_df).transform(prop_final_df)\n",
        "  prop_final_df = discretizer_centile.fit(prop_final_df).transform(prop_final_df)\n",
        "  prop_final_df = prop_final_df.withColumn('score_decile', prop_final_df['score_decile'] + lit(1)) . \\\n",
        "                                withColumn('score_centile', prop_final_df['score_centile'] + lit(1)) .\\\n",
        "                                withColumn('orig_score_value', prop_final_df['probability'])\n",
        "  prop_final_df = prop_final_df.withColumn('high_score_ind', when(prop_final_df['score_centile'] > 75, 'Y').otherwise('N'))\n",
        "  prop_final_df = prop_final_df.drop('probability')\n",
        "  prop_final_df = prop_final_df.select(*finalCols)\n",
        "\n",
        "  # Saving Scoring into the final table\n",
        "  prop_final_df.createOrReplaceTempView('accessorypurchasedig_predictions_final')\n",
        "  hiveContext.sql('drop table if exists' + outtable)\n",
        "  print('out table name: ' + outtable)\n",
        "  hiveContext.sql('create table ' + outtable + ' as select * from accessorypurchasedig_predictions_final')\n",
        "  print(outtable + ' table is created')\n",
        "\n",
        "propensity_pred = propensity_predictions(pipeline_model = pipeline_model, serialized_objects = serialized_objects, df = feat_df,\n",
        "                                         spark = spark, hiveContext = hivecontext, outtable=outtable)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}